2025-07-11 12:20:23,486 | rank:0 | gpu_logger | INFO | GPU0: 7530.2/81920.0 MB (9.2%) | RAM: 129.3/2015.7 GB (11.4%) | Loading datasets from [{'name_or_path': 'rd211/Big-Math-RL-Verified-Filtered', 'split': 'train', 'ratio': 1.0}]
No validation datasets provided or an error occurred while loading them.
Key 'eval_datasets' not in 'DatasetConfig'
    full_key: dataset.eval_datasets
    reference_type=DatasetConfig
    object_type=DatasetConfig
2025-07-11 12:20:27,090 | rank:0 | gpu_logger | INFO | GPU0: 7530.2/81920.0 MB (9.2%) | RAM: 129.3/2015.7 GB (11.4%) | Loaded 10000 training examples
Applying template (num_proc=4): 100%|███████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 8256.69 examples/s]
[2025-07-11 12:20:29,673] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 4
Error executing job with overrides: []
Traceback (most recent call last):
  File "/workspace/PedagogicalRL/train_rl.py", line 114, in main
    trainer = ClassroomGRPOTrainer(
              ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/PedagogicalRL/src/grpo/trainer.py", line 308, in __init__
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py", line 311, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py", line 4758, in from_pretrained
    config = cls._autoset_attn_implementation(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py", line 2315, in _autoset_attn_implementation
    cls._check_and_enable_flash_attn_2(
  File "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py", line 2457, in _check_and_enable_flash_attn_2
    raise ImportError(f"{preface} the package flash_attn seems to be not installed. {install_message}")
ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
